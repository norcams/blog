<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>norcams</title><link>http://blog.norcams.org/</link><description></description><atom:link href="http://blog.norcams.org/feeds/kristian-skurtveit.rss.xml" rel="self"></atom:link><lastBuildDate>Tue, 16 Jun 2015 19:21:00 +0200</lastBuildDate><item><title>Video of bachelor thesis</title><link>http://blog.norcams.org/video-of-bachelor-thesis.html</link><description>&lt;p&gt;This is a video that presents the core components of the &lt;a class="reference external" href="http://openstack.b.uib.no/2015/04/20/day-14-summary-collecting-openstack-logs-with-logstash/"&gt;bachelor
thesis&lt;/a&gt;.
Thanks to &lt;a class="reference external" href="http://remix.kwed.org/download.php/2492/Christian%20Schuler%20-%20Druid%20version%20%28grassroots%20mix%29.mp3"&gt;Christian
Schüler&lt;/a&gt;
for allowing me to borrow his Druid version (grassroots mix) for
background music.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2015/06/Monitorering-av-OpenStack-Kristian-Adlandsvik-Skurtveit.mp4"&gt;See the video here&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Tue, 16 Jun 2015 19:21:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-06-16:video-of-bachelor-thesis.html</guid><category>video</category></item><item><title>Prosjektrapport bacheloroppgave</title><link>http://blog.norcams.org/prosjektrapport-bacheloroppgave.html</link><description>&lt;p&gt;Da har jeg fullført bacheloroppgaven min. Endelig prosjektrapport ligger
nå ute for nedlastning her: &lt;a class="reference external" href="http://openstack.b.uib.no/files/2015/06/Monitorering-av-OpenStack-ved-UiB.pdf"&gt;Monitorering av OpenStack ved
UiB&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Mon, 08 Jun 2015 10:11:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-06-08:prosjektrapport-bacheloroppgave.html</guid><category>prosjektrapport</category></item><item><title>Dag 35-39: Skriveuke</title><link>http://blog.norcams.org/dag-35-39-skriveuke.html</link><description>&lt;p&gt;Gjennom hele uken har tiden gått med til å skrive ferdig utkast til
prosjektrapporen. Denne ble levert inn 26. mai og endelig
innleveringsfrist er 9. juni. Arbeidet som gjenstår er å skrive
dokumentasjon om det endelige monitoreringsoppsettet slik at dette
enkelt kan tas i bruk. Dette forventes å være ferdig i neste uke.&lt;/p&gt;
&lt;p&gt;Oppsummert har det i under uttestingen av Logstash blitt prosessert
8,943,279 millioner hendelser som har blitt lagret i Elasticsearch og
visualisert gjennom Kibana og Graphite.&lt;/p&gt;
&lt;p&gt;Endelig innleveringsdato for prosjektrapporten er 9. juni..&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Thu, 28 May 2015 09:21:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-05-28:dag-35-39-skriveuke.html</guid><category>elasticsearch</category><category>kibana</category><category>logstash</category></item><item><title>Dag 30-34: Grafing av API responskoder &amp; responstider</title><link>http://blog.norcams.org/dag-30-34-grafing-av-api-responskoder-responstider.html</link><description>&lt;p&gt;Siden vi allerede ekstraherer alle loggmeldinger som genereres i
OpenStack kan vi også hente ut og visualisere API-responskoder og
API-responstider.&amp;nbsp; Dette grafes på følgende måte: &lt;a class="reference external" href="http://openstack.b.uib.no/files/2015/05/api-responses.png"&gt;&lt;img alt="api-responses" src="http://openstack.b.uib.no/files/2015/05/api-responses.png" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Med en API responstid følger også en responskode.&amp;nbsp; For å&amp;nbsp; se hvor mange
ganger responskodene forekommer i forhold til hverandre kan dette
visualiseres på denne måten:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2015/05/total-api-response-codes.png"&gt;&lt;img alt="total api-response-codes" src="http://openstack.b.uib.no/files/2015/05/total-api-response-codes.png" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Etter helgen 17. mai vil jeg hovedsaklig fokusere på å fullføre et
utkast til prosjektrapporten som skal inn den 26. mai. Endelig
innleveringsfrist er satt til 9 juni og prosjektrapporten vil
selvfølgelig bli publisert på bloggen.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Thu, 21 May 2015 11:26:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-05-21:dag-30-34-grafing-av-api-responskoder-responstider.html</guid><category>api responskoder</category><category>prosjektrapport</category></item><item><title>Dag 25-29: Oppsett av dashing-ceph/openstack, rydding av config</title><link>http://blog.norcams.org/dag-25-29-oppsett-av-dashing-cephopenstack-rydding-av-config.html</link><description>&lt;p&gt;Den siste uken har gått med til å sette opp dashing-ceph,
dashing-openstack, rydding av Logstash config og oppdatering av
bacheloroppgaven. Dashing er kort fortalt et dashbord rammeverk for å
visualisere informasjon i &amp;quot;fine bokser&amp;quot;, se bilde nedenfor. Det er mye
ting i luften akkurat nå og det kommer til å bli en hektisk tid fremover
mot innleveringen andre uken i juni.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2015/05/dashing-ceph.png"&gt;&lt;img alt="dashing-ceph" src="http://openstack.b.uib.no/files/2015/05/dashing-ceph.png" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bachelorprosjektet skal framføres enten den 10, 11 eller 15 juni og alle
som er interessert må komme å høre på! Planen er å ha en enkel men
oversiktlig presentasjon med live demo som viser hva jeg har jobbet med
dette semesteret. I uken som kommer vil de siste tekniske bitene bli
implementert før skriveperioden starter for fullt.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Mon, 11 May 2015 12:22:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-05-11:dag-25-29-oppsett-av-dashing-cephopenstack-rydding-av-config.html</guid><category>ceph</category><category>dashing</category><category>openstack</category><category>oppgave</category></item><item><title>Dag 20-24: Grafing av instansdata</title><link>http://blog.norcams.org/dag-20-24-grafing-av-instansdata.html</link><description>&lt;p&gt;Etter å ha eksperimentert med ulike metrics fra Logstash og statsd i
forrige uke har jeg laget noen enkle python scripts som spør keystone
databasen ved jevne mellomrom for instansdata. Antallet kjørende
instanser, slettede instanser, instanser som har feilet, samt type
instans blir nå grafet i Grafana.&lt;/p&gt;
&lt;p&gt;Grunnen for dette er at vi skal kunne holde en enkel oversikt over&amp;nbsp;alle
instansene og deres status. I tillegg skal vi kunne kartlegge fremtidige
ressursbehov dersom totalkapasiteten i systemet er i ferd med å bli
nådd. Dette går under kategorien proaktiv overvåking, og vi kan løse
ressursbehov ved å legge til mer ressurser&amp;nbsp;under drift istedenfor når
systemet har nådd sin totale kapasitet.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2015/04/metrics-grafer.png"&gt;&lt;img alt="metrics-grafer" src="http://openstack.b.uib.no/files/2015/04/metrics-grafer.png" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2015/05/instans-graf.png"&gt;&lt;img alt="instans-graf" src="http://openstack.b.uib.no/files/2015/05/instans-graf.png" /&gt;&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Mon, 04 May 2015 12:58:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-05-04:dag-20-24-grafing-av-instansdata.html</guid><category>instanser</category><category>keystone</category><category>nova</category></item><item><title>Dag 15-19: Metrics gjennom graphite og statsd</title><link>http://blog.norcams.org/dag-15-19-metrics-gjennom-graphite-og-statsd.html</link><description>&lt;p&gt;Har gjennom hele uken eksperimentert med å lage metrics utav loggene som
kan sendes fra logstash til Graphite for grafing. Det som kan grafes så
langt er tilgjengelige ressurser på alle compute nodene i OpenStack.
Tanken er at disse grafene skal kunne eksponeres ut mot brukerne slik at
en kan se hvor mye ressurser det er tilgjengelig til enhver tid på hver
av nodene.&lt;/p&gt;
&lt;p&gt;I tillegg har jeg laget noen python scripts som skal brukes til å hente
ut spesifikke instansdata som også skal kunne grafes. Videre skal jeg
også se på muligheten til å hente ut data fra Ceilometer.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Sat, 02 May 2015 14:05:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-05-02:dag-15-19-metrics-gjennom-graphite-og-statsd.html</guid><category>grafer</category><category>graphite</category></item><item><title>Day 14: Summary: Collecting OpenStack logs with Logstash</title><link>http://blog.norcams.org/day-14-summary-collecting-openstack-logs-with-logstash.html</link><description>&lt;p&gt;This blog post is a summary of the first 14 days of work on my bachelor
degree.&amp;nbsp; It is written in English to satisfy some of our Brazilian
readers at the University in São Paulo.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.openstack.org/"&gt;Openstack&lt;/a&gt; consists of many different
services and components, and&amp;nbsp; all of these services are logging
information to their own log files respectively.&amp;nbsp; However, it would be
an impossible job for a system administrator to monitor all of these log
files by simply tailing them through the terminal. This is where
Logstash is useful. Logstash is an open-source tool for managing events
and logs. It is primarily used for collecting logs, parsing them and
save them for later use. The tool also comes with an interface for
searching in the logs you've collected.&lt;/p&gt;
&lt;p&gt;Based on the &lt;a class="reference external" href="https://github.com/norcams/winch"&gt;winch&lt;/a&gt; project on
GitHub I have created a Logstash node where all logs coming from
OpenStack have been centralized. On this node all logs are parsed,
information extracted and saved in Elasticsearch. Seconds after, the
extracted information is visible on the Kibana dashboard (a front-end to
Elasticsearch) ready for searching, filtering and visualization.
Extracting the information from the log files is a bit more complex than
it sounds. However, Logstash is very easy to get started with and once
the basics are covered you're ready to write complex filters yourself.
Having the &lt;a class="reference external" href="http://grokdebug.herokuapp.com/"&gt;grok debugger&lt;/a&gt; in hand
and a quick
&lt;a class="reference external" href="http://www.google.no/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CB4QtwIwAA&amp;amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYIKm6WUgFTY&amp;amp;ei=coQyVeHILYOtsAGbnoGADQ&amp;amp;usg=AFQjCNHNCs05hUDcTnuyPAcKuiPltMC--A&amp;amp;sig2=62L8IW-f5ply9FM_TA7eUg&amp;amp;bvm=bv.91071109,d.bGg"&gt;tutorial&lt;/a&gt;
in the other also helps :)&lt;/p&gt;
&lt;p&gt;In my configuration I'm pretty much finished with the filters that
covers all the lines of log that the nova services in OpenStack are
generating. Launching, rebooting, deleting instances and error messages
related to instances is now hit by a filter in Logstash and saved for
later searches. Additionally I've made a filter that caches everything
that is not matched by any previous filter in the configuration. This is
in case some special event should occur or if the system goes haywire
(not that I expect that to happen). The 'all-matching' filter is tagged
with &amp;quot;unmatched_event&amp;quot; , and from here we can go back and change the
original filter to take &amp;quot;these&amp;quot; special events into account. By doing
this we will at all times have an overview if something should go wrong.
Also we won't miss any data that somewhat could be important for us to
know. The Logstash configuration can be found
&lt;a class="reference external" href="https://github.com/norcams/winch/blob/stable/icehouse-centos6-monitoring/conf/logstash.conf"&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Further on I've also created some metrics which can been seen in the
configuration file. Winch monitoring also consist of a Graphite node
where these metrics are sent and visualized. I believe that some data
are best when they are graphed in some way or the other providing an
overview on a day-to-day basis (or even minute-to-minute basis)&amp;nbsp; on how
the system is performing. Graphs also helps seeing systems in context
which is very useful.&lt;/p&gt;
&lt;p&gt;During the next couple of weeks I will continue to make filters, extract
data from logs until all OpenStack services are covered, visualize data
and put data in context by making graphs and much more. Stay tuned!&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Mon, 20 Apr 2015 10:39:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-04-20:day-14-summary-collecting-openstack-logs-with-logstash.html</guid><category>elasticsearch</category><category>logstash</category><category>monitoring</category></item><item><title>Dag 13: Grafing av disk, cpu og minnebruk</title><link>http://blog.norcams.org/dag-13-grafing-av-disk-cpu-og-minnebruk.html</link><description>&lt;p&gt;Metrics til graphite blir sendt på et spesifikt format. Dette er
standard uansett hva system man bruker for å lage metrics. &amp;nbsp;Her
spesifiseres først navnet, deretter verdien og til slutt datoen.
Eksempelvis:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
echo &amp;quot;test.bash.stats 42 `date +%s`&amp;quot; | nc graphite.example.com 2003
&lt;/pre&gt;
&lt;p&gt;Dette vil ikke gi et stort utslag på en graf, men når man sender data
over tid vil man på sikt kunne se at det gir utslag. Siden logstash
konfigurasjonen henter informasjon om disk, cpu- og minnebruk fra
loggfilene kan dette sendes videre for visualisering.&amp;nbsp;Bildet under er
visualiserte data basert på denne
&lt;a class="reference external" href="http://paste.debian.net/167281/"&gt;konfigurasjonen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2015/04/metrics-grafer.png"&gt;&lt;img alt="metrics-grafer" src="http://openstack.b.uib.no/files/2015/04/metrics-grafer.png" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bildet viser tre bokser som visualiserer tilgjengelige ressurser.
Diskboksen er også konfigurert slik at den endrer farge basert på hvor
mye diskplass som er tilgjengelig på disken.&amp;nbsp;Dette er en god begynnelse!
I morgen og ut i neste uke kommer jeg til å fortsette med datainnsamling
og filtere i Logstash. Følg med!&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Fri, 17 Apr 2015 12:04:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-04-17:dag-13-grafing-av-disk-cpu-og-minnebruk.html</guid><category>graphite</category><category>logstash</category><category>metrics</category></item><item><title>Dag 12: Kartlegging og sending av metrics</title><link>http://blog.norcams.org/dag-12-kartlegging-og-sending-av-metrics.html</link><description>&lt;p&gt;Metrics som vi kan sende til visualiseringssystemer er data. Informasjon
som representerer en eller annen verdi kan grafes og visualiseres og på
denne måten gi oss oversikt over hvordan systemet fungerer til enhver
tid. Dagen i dag har for det meste blitt brukt til å lese dokumentasjon
og teste ulike fremgangsmåter på hva metrics jeg ønsker å ha med og
hvordan disse dataene skal sendes og visualiseres.&lt;/p&gt;
&lt;p&gt;Metrics er ikke så veldig bra dokumentert på Logstash sine
&lt;a class="reference external" href="http://logstash.net/docs/1.4.2/filters/metrics"&gt;nettsider&lt;/a&gt;. I
tillegg er de aller fleste eksempler på metrics er basert på å hente ut
informasjon fra apache-aksesslogger. Siden jeg skal hente ut mer data
enn dette blir det mye prøving og feiling fremover på å få metrics til å
fungere på den måten jeg vil. Mer om dette i morgen!&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Fri, 17 Apr 2015 08:42:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-04-17:dag-12-kartlegging-og-sending-av-metrics.html</guid><category>logstash</category><category>metrics</category></item><item><title>Dag 11: Generering av testgrafer</title><link>http://blog.norcams.org/dag-11-generering-av-testgrafer.html</link><description>&lt;p&gt;Noe mer konfigurasjon måtte til for at den nye grafnoden skulle fungere
som den skulle. Provisjonering av noden måtte kjøres to ganger, i
tillegg til at en rekke småting måtte konfigureres manuelt. Med alle
disse hindringene til side var det tid for å generere noen grafer.&lt;/p&gt;
&lt;p&gt;Ved hjelp av et script som heter ceilometer publisher har jeg hatt
mulighet til å pushe enkelte data direkte fra ceilometer og inn til
Graphite. Data som cpu- og minnebruk har vært de to mest aktuelle, og
ved å opprette instanser i OpenStack har jeg kunne sett at grafene
utvikler seg over tid. Dette gir et interessant overblikk over
tilgjengelige ressurser på systemet og hjelper oss å se data i
sammenheng. Samtidig som det også tilbyr systemadministratorer proaktiv
overvåkning ved å kunne forutse og hindre problemer før de oppstår.&lt;/p&gt;
&lt;p&gt;Når man skal sette opp grafer i Graphite/Grafana baserer man seg på
såkalte metrics. Hvis man for eksempel ønsker å se på CPU bruk på en
maskin het metric'en i vårt tilfelle:
carbon.agents.graphite_winch_local-a.cpuUsage. Denne visualiseres
slik:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2015/04/cpu-bruk.png"&gt;&lt;img alt="cpu-bruk" src="http://openstack.b.uib.no/files/2015/04/cpu-bruk.png" /&gt;&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Tue, 14 Apr 2015 12:56:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-04-14:dag-11-generering-av-testgrafer.html</guid><category>grafana</category><category>graphite</category><category>winch</category></item><item><title>Dag 10: Oppsett av Graphite og Grafana</title><link>http://blog.norcams.org/dag-10-oppsett-av-graphite-og-grafana.html</link><description>&lt;p&gt;Logstash, Kibana og Elasticsearch kjører per i dag på en egen node i
winch. For at lasten ikke skal bli for stor på denne virtuelle noden
hadde jeg i første omgang tenkt å lage en ny node for
grafvirtualisering. Grafnoden skal kjøre verktøyet Graphite, som støtter
metrics som kommer fra Logstash. I tillegg skal vi benytte oss av en
annen frontend enn det Graphite tilbyr og derfor skal også verktøyet
Grafana installeres.&lt;/p&gt;
&lt;p&gt;Jeg benytter meg av puppetmodulene til
&lt;a class="reference external" href="https://github.com/echocat?utf8=%E2%9C%93&amp;amp;query=puppet-"&gt;echocat&lt;/a&gt;
siden disse er godt vedlikeholdte og konfigurasjonen av puppetkoden var
rett fram. Det meste gikk greit for seg og vagrantboksen har en
oppstart- og installasjonstid på mellom 3 og 4 minutter.&amp;nbsp; For de som er
interessert i å se litt på koden og øvrige detaljer rundt oppsettet av
vagrantboksen kan ta en kikk på
&lt;a class="reference external" href="https://github.com/norcams/winch/blob/stable/icehouse-centos6-monitoring/puppet/manifests/graphite.pp"&gt;winch&lt;/a&gt;
repoet.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2015/04/provision-graphite.png"&gt;&lt;img alt="provision-graphite" src="http://openstack.b.uib.no/files/2015/04/provision-graphite.png" /&gt;&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Tue, 14 Apr 2015 12:47:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-04-14:dag-10-oppsett-av-graphite-og-grafana.html</guid><category>grafana</category><category>logstash</category><category>puppet</category><category>winch</category></item><item><title>Dag 9: Ferdig med eksamen</title><link>http://blog.norcams.org/dag-9-ferdig-med-eksamen.html</link><description>&lt;p&gt;I dag var siste eksamen på Høgskolen og resten av dagen jobbet jeg på
UiB. Nå som det finnes en god del informasjon som sendes til Logstash er
det på tide å se på hvordan denne informasjonen kan visualiseres. I
Logstash kan man lage metrics av informasjon som er kommet inn, og dette
kan sendes videre til ulike grafverktøy for visualisering. Dette er noe
jeg ønsker for at vi skal kunne se data i sammenheng og kunne forutse
problemer før de oppstår. Ettersom jeg allerede har funnet et
eksempeloppsett på en vagrantboks som installerer to grafverktøy kommer
jeg til å se videre på dette på mandag.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Tue, 14 Apr 2015 11:51:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-04-14:dag-9-ferdig-med-eksamen.html</guid><category>logstash</category><category>visualisering</category></item><item><title>Dag 8: Filtrering av unyttig informasjon</title><link>http://blog.norcams.org/dag-8-filtrering-av-unyttig-informasjon.html</link><description>&lt;p&gt;Ikke alle data som kommer inn gjennom Logstash er nyttige data. Data som
ikke gir noen nyttig informasjon eller rett og slett bare er støy er
nødt til å skjules eller filtreres bort. I Logstash kan alle datafelter
som blir opprettet når man lager filter søkes i. På bakgrunn av dette
kan man ved hjelp av regulære uttrykk søke etter informasjon man vil
filtrere bort slik at dette ikke kommer med. Hvis man for eksempel ikke
skulle ønske å se alle infomeldinger som et system genererer kan dette
filtreres bort på denne måten.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="100%" /&gt;
&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;&lt;p class="first"&gt;if &amp;quot;INFO&amp;quot; in [openstack_loglevel] {drop {}&lt;/p&gt;
&lt;/p&gt;&lt;p&gt;&lt;p class="last"&gt;}&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Videre vil jeg ta en fullstendig gjennomgang av hva informasjon som skal
filtreres bort slik at dette ikke overskygger viktige data i henhold til
problemstillingen.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Tue, 31 Mar 2015 20:08:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-03-31:dag-8-filtrering-av-unyttig-informasjon.html</guid><category>grok</category><category>logstash</category><category>regulære uttrykk</category></item><item><title>Dag 7: Filtrering av instanshendelser</title><link>http://blog.norcams.org/dag-7-filtrering-av-instanshendelser.html</link><description>&lt;p&gt;Dagen ble benyttet til å lage grok-filtre som henter ut informasjon fra
OpenStack-instanser. Om en instans blir opprettet, slettet, rebootet,
re-initialisert, utvidet eller skulle få en feil vil dette bli truffet
av filteret og vi vil kunne se denne hendelsen og all informasjon i
webpanelet Kibana.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2015/03/logstash-output.png"&gt;&lt;img alt="logstash-output" src="http://openstack.b.uib.no/files/2015/03/logstash-output-300x129.png" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Her ser vi at all informasjon som jeg publisert i forrige bloggpost har
kommet inn som egne felter. Disse kan nå søkes opp i og en kan
visualisere dette på forskjellige måter. Neste steg blir å se på andre
data i OpenStack og hvordan vi kan hente ut informasjon om opprettelser
av eksempelvis nettverk og rutere.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Tue, 31 Mar 2015 19:49:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-03-31:dag-7-filtrering-av-instanshendelser.html</guid><category>elasticsearch</category><category>logstash</category><category>neutron</category></item><item><title>Dag 6: Grok-filtre i Logstash</title><link>http://blog.norcams.org/dag-6-grok-filtre-i-logstash.html</link><description>&lt;p&gt;For å kunne hente ut informasjon fra data som kommer inn i Logstash kan
man benytte seg av grok-filtre. Grok-filtre er bygget på toppen av
regulære uttrykk og på bakgrunn av dette kan man hente ut den
informasjonen man ønsker, eksempelvis fra loggdata. For å hente ut
relevante data er man avhengig av å vite noe om dataene på forhånd. For
eksempel sier følgende logglinje hvilken dato hendelsen skjer på,
hvilket loglevel informasjonen er i, hvilken tjeneste som genererer
meldingen og hvilken instans i OpenStack meldingen omhandler. At en
instans starter kan være nyttig informasjon dersom den ikke skulle komme
opp som forventet.&lt;/p&gt;
&lt;p&gt;2015-03-20T08:38:51+00:00 compute 2015-03-20 08:38:51.292 11139 AUDIT
nova.compute.manager [req-414b1736-9bf6-4457-a848-1295ebb12d7c
b251c86204eb44b6822a998da0d28ad4 1a49bb7c49914d96b95ade9b1345eac2]
[instance: 4e86c611-0914-4da2-9ed4-4c2ca5529ffb] Rebooting instance&lt;/p&gt;
&lt;p&gt;Ved hjelp av &lt;a class="reference external" href="http://grokdebug.herokuapp.com/"&gt;grok debugger&lt;/a&gt;har
jeg laget et &lt;a class="reference external" href="http://paste.debian.net/164129/"&gt;filter&lt;/a&gt; som henter ut
informasjonen og gjør den svært oversiktlig og enkel å organisere. Etter
at informasjonen er kommet gjennom filteret ser den slik ut:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
{
  &amp;quot;openstack_hostname&amp;quot;: [
    &amp;quot;compute&amp;quot;
  ],
  &amp;quot;timestamp&amp;quot;: [
    &amp;quot;2015-03-20 08:38:51.292&amp;quot;
  ],
  &amp;quot;openstack_pid&amp;quot;: [
    &amp;quot; 11139&amp;quot;
  ],
  &amp;quot;openstack_loglevel&amp;quot;: [
    &amp;quot;AUDIT&amp;quot;
  ],
  &amp;quot;openstack_program&amp;quot;: [
    &amp;quot;nova.compute.manager &amp;quot;
  ],
  &amp;quot;request_id_list&amp;quot;: [
    &amp;quot;414b1736-9bf6-4457-a848-1295ebb12d7c&amp;quot;,
    &amp;quot;1a49bb7c49914d96b95ade9b1345eac2&amp;quot;
  ],
  &amp;quot;openstack_instance_id&amp;quot;: [
    &amp;quot;4e86c611-0914-4da2-9ed4-4c2ca5529ffb&amp;quot;
  ],
  &amp;quot;openstack_instance_action&amp;quot;: [
    &amp;quot;Rebooting instance&amp;quot;
  ]
}
&lt;/pre&gt;
&lt;p&gt;Dette sendes så videre fra Logstash til Elasticsearch der man kan søke
opp spesifikke data og visualisere dette på mange ulike måter.
Visualisering vil bli et eget tema på bloggen senere.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Tue, 31 Mar 2015 14:58:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-03-31:dag-6-grok-filtre-i-logstash.html</guid><category>elasticsearch</category><category>grok</category><category>informasjon</category><category>logstash</category></item><item><title>Dag 5: Start av fysisk testmiljø</title><link>http://blog.norcams.org/dag-5-start-av-fysisk-testmiljo.html</link><description>&lt;p&gt;Som tidligere publisert på bloggen har jeg fått tildelt en 32GB fysisk
blade server der winch har blitt installert. Hensikten med dette er å
kjøre et virtuelt OpenStack-testmiljø der jeg kan teste ulike
monitoreringssystem. For øyeblikket er jeg i gang med å teste Logstash,
Elasticsearch og Kibana og har satt opp dette i winch under en egen
monitoreringsbranch. I denne branchen har jeg laget en logstash node som
er konfigurert til å ta imot alle loggdata som kommer fra OpenStack.&lt;/p&gt;
&lt;p&gt;Her vil jeg ha muligheten til å se hvilke data som kommer inn slik at
jeg kan hente ut den informasjonen som er relevant. Samtidig ønsker jeg
å kunne filtrere vekk all unyttig informasjon slik at dette ikke
overskygger viktige data. Jeg har laget en vagrantboks som installerer
Logstash automatisk ved hjelp av puppet. Dersom noe feil skulle skje
eller testoppsettet ikke skulle fungere som det skal, kan jeg enkelt
slette og starte maskinen på nytt for å komme tilbake til der jeg var
før feilen skjedde. Ved hjelp av dette sparer jeg mye tid og kan
fokusere mer på testingen av de ulike verktøyene. Vagrantboksen er
definert
&lt;a class="reference external" href="https://github.com/norcams/winch/blob/stable/icehouse-centos6-monitoring/puppet/manifests/logstash.pp"&gt;slik&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Tue, 31 Mar 2015 13:54:00 +0200</pubDate><guid>tag:blog.norcams.org,2015-03-31:dag-5-start-av-fysisk-testmiljo.html</guid><category>logstash</category><category>monitorering</category><category>vagrant</category><category>winch</category></item><item><title>Dag 4: Videre konfigurasjon av winch</title><link>http://blog.norcams.org/dag-4-videre-konfigurasjon-av-winch.html</link><description>&lt;p&gt;Dagen ble benyttet til å teste at provisjoneringen av controller og
compute fungerer sammen med logstash. Det er mye av konfigurasjonen som
utføres manuelt for øyeblikket. Blant annet må alle OpenStack tjenestene
manuelt endres for at de skal logge til syslog og sende til logstash
noden som tar imot og parser loggende.&lt;/p&gt;
&lt;p&gt;Førsteprioritet er uansett å få opp et fysisk testmiljø der jeg har
muligheten til å starte flere instanser og sjekke at all informasjon som
blir generert i loggfilene blir samlet og håndtert. Videre må
nøkkelfunksjonalitet i systemet testes på en slik måte at loggdataene
som blir generert kan si noe om hvilken tilstand systemet er i. Om
tjenester er oppe og går, om det har forekommet feil den siste tiden
osv.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Sat, 14 Mar 2015 23:13:00 +0100</pubDate><guid>tag:blog.norcams.org,2015-03-14:dag-4-videre-konfigurasjon-av-winch.html</guid><category>blade server</category><category>winch</category></item><item><title>Dag 3: Submoduler i git og installasjon på blade server</title><link>http://blog.norcams.org/dag-3-submoduler-i-git-og-installasjon-pa-blade-server.html</link><description>&lt;p&gt;I forrige uke fikk jeg problemer med å legge til enkelte puppetmoduler
til monitoreringsbranchen på github. Etter en del feilsøking endte jeg
opp med å legge disse til som submoduler. Fordelen med submoduler er at
man slipper å måtte oppdatere modulene i sitt eget repository. Ved å
kjøre kommandoen nedenfor vil modulen lastes ned, og det vil ligge en
sti til det remote repositoriet i .gitmodules.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
git submodule add https://github.com/elastic/puppet-logstash
&lt;/pre&gt;
&lt;p&gt;I tillegg til submodulene er det også blitt lagt til en egen mappe for
alle konfigurasjonsfiler og patterns til logstash. Rsyslog.conf ligger
også her, men planen videre er å legge til rsyslog puppetmodulen slik at
det aller meste kan styres gjennom puppet. Jeg begynte også å installere
winch på blade serveren jeg har fått tildelt. Denne serveren har 32GB
med minne og jeg har derfor mulighet til å ha kjørende en god del
instanser som jeg skal teste med. Mer om dette i morgen!&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Sat, 14 Mar 2015 21:45:00 +0100</pubDate><guid>tag:blog.norcams.org,2015-03-14:dag-3-submoduler-i-git-og-installasjon-pa-blade-server.html</guid><category>logstash</category><category>puppet</category><category>rsyslog</category></item><item><title>Dag 2: Integrering av logstash i "winch"</title><link>http://blog.norcams.org/dag-2-integrering-av-logstash-i-winch.html</link><description>&lt;p&gt;Under utplasseringen var jeg med på å lage et automatisk oppsett som
installerte OpenStack og tilhørende komponenter. Dette ble gjort ufifra
et prosjekt på github som het
&lt;a class="reference external" href="https://github.com/norcams/winch"&gt;winch&lt;/a&gt;. Som jeg tok opp i min
forrige bloggpost ønsker jeg å samle alle OpenStack loggene på en og
samme maskin. Derfor her jeg laget en ny branch i winch som inkluderer
en monitoreringsnode. Denne noden vil være ansvarlig for alt som skjer
med tanke på logginnsamling, parsing og visualisering av informasjon. Da
er det viktig at verktøyene som skal gjøre dette blir installert på
samme måte som tidligere, ved hjelp av puppetmoduler.&amp;nbsp; Da kan vi sette
en ønsket tilstand og si at den nye noden skal være en
monitoreringsnode.&lt;/p&gt;
&lt;p&gt;Arbeidet med å puppetifisere alle verktøyene er kommet godt i gang. Har
laget en manifest fil for logstash som kan sees
&lt;a class="reference external" href="https://github.com/norcams/winch/blob/stable/icehouse-centos6-monitoring/puppet/manifests/logstash.pp"&gt;her&lt;/a&gt;.
Denne vil benytte seg av puppet modulene lokalisert i puppet/modules og
installere verktøyene med konfigurasjonsfiler spesifisert i manifestet.
Det som gjenstår for at integreringen skal være komplett er å legge til
de puppetmodulene jeg trenger. Disse er submoduler og blir derfor lagt
til annerledes. Mer om dette i neste bloggpost.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Sat, 07 Mar 2015 16:56:00 +0100</pubDate><guid>tag:blog.norcams.org,2015-03-07:dag-2-integrering-av-logstash-i-winch.html</guid><category>github</category><category>logstash</category><category>puppet</category><category>winch</category></item><item><title>Dag 1: Installasjon og oppsett av logstash, elasticsearch &amp; kibana</title><link>http://blog.norcams.org/dag-1-installasjon-og-oppsett-av-logstash-elasticsearch-kibana.html</link><description>&lt;p&gt;Etter tips fra prosjektleder i UH-sky prosjektet er et av
monitoreringsverktøyene jeg har valgt å se på
&lt;a class="reference external" href="http://logstash.net/"&gt;logstash&lt;/a&gt;. Et&amp;nbsp; kraftig&amp;nbsp; verktøy som brukes til
å håndtere hendelser og logger.&lt;/p&gt;
&lt;p&gt;&lt;img alt="logstash" src="http://logstash.net/images/logstash.png" /&gt;&lt;/p&gt;
&lt;p&gt;Logstash samler inn logger fra ulike kilder, parser loggene og lagrer de
til senere bruk.&amp;nbsp; Når man installerer logstash kommer det også med et
webinterface der man kan søke etter hendelsene i loggfilene og
visualisere dette slik man ønsker. Dette for å kunne kartlegge feil, se
endringer i systemer over tid, sette sammen data som har påvirkning på
herandre osv. Logstash er et åpent kildekodeverktøy og er lisensiert
under Apache 2.0.&lt;/p&gt;
&lt;p&gt;OpenStack er et komplekst rammeverk som består av mange tjenester. Hver
tjeneste har sitt eget bruksområde og sitt eget API. Det er mye
informasjon som til daglig vil være spredt rundt om i&amp;nbsp; systemet. Dette
gjør det viktig å samle all loggdata på et samlet sted slik at det blir
enklere å hente ut den informasjonen vi trenger for å sørge for at
rammeverket til enhver tid fungerer som det skal. Logstash er veldig
egnet til dette formålet. I logstash.conf kan vi tagge innkommende
logger og videre kan det så kjøres filter basert på disse taggene for å
hente ut den spesifikke informasjonen vi vil ha tak i. For eksempel
tjenestenavn, diskbruk, antall påloggingsforsøk, brukere, IP adresser,
nettleser osv. Alt av informasjon som finnes i en loggfil kan
ekstraheres, lagres og videresendes til en eller annen form for
visualisering.&lt;/p&gt;
&lt;p&gt;Jeg har til nå arbeidet med en kodebase som jeg forket på github som
installerer disse tre verktøyene som nevnt i overskriften. Videre har
jeg tenkt å integrere denne mot winch slik at den passer øvrig
prosjektstruktur. Under følger et bilde av hvordan visualiseringen av
loggdataene ser ut.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2015/03/kibana4-visualisering.png"&gt;&lt;img alt="kibana4-visualisering" src="http://openstack.b.uib.no/files/2015/03/kibana4-visualisering-300x171.png" /&gt;&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Sat, 07 Mar 2015 16:34:00 +0100</pubDate><guid>tag:blog.norcams.org,2015-03-07:dag-1-installasjon-og-oppsett-av-logstash-elasticsearch-kibana.html</guid><category>elasticsearch</category><category>github</category><category>kibana4</category><category>logstash</category></item><item><title>Dag 0: Oppsummering</title><link>http://blog.norcams.org/dag-0-oppsummering.html</link><description>&lt;p&gt;Denne bloggposten er en oppsummering av arbeidet som har blitt gjort før
forprosjektet i bacheloroppgaven har kommet i gang.&lt;/p&gt;
&lt;p&gt;I henhold til problemstillingen skal jeg kartlegge forskjellige
monitoreringsverktøy og teste bruken av disse. Dette er arbeid som jeg
har kommet godt i gang og jeg har fått tilegnet med rimelig god oversikt
over forskjellige
&lt;a class="reference external" href="https://wiki.openstack.org/wiki/Operations/Tools"&gt;verktøy&lt;/a&gt; som
eksisterer for bruk i OpenStack per i dag. Videre skal jeg også belyse
fordeler og ulemper med forskjellige overvåkningsverktøy. Hva passer
best til vårt bruk? Er noen verktøy bedre for sky enn for tradisjonell
bruk?
Så langt kan jeg se klare fordeler med enkelte verktøy som jeg linket
til tidligere. Verktøene er godt vedlikeholdte, populære og de er alle
av åpen kildekode. Sistnevnte punkt tillater meg i aller største grad å
spesialtilpasse verktøyene til mitt formål. Jeg har muligheten til å få
ut den informasjonen som er av relevans for å kunne identifisere og løse
problemer som oppstår. I tillegg til at jeg veldig enkelt kan tagge
informasjon som ikke er av relevans som unødvendig slik at dette ikke
overskygger faktiske problemer som eventuelt kan forekomme.&lt;/p&gt;
&lt;p&gt;Dette har gjort jobben med å finne et verktøy som passer
problembeskrivelsen noe enklere. Det er ikke alle verktøy en vil ha
mulighet til å spesialtilpasse i så stor grad, og disse verktøyene vil
naturligvis bli valgt bort.&lt;/p&gt;
&lt;p&gt;Jeg også fått tildelt en egen blade server der uttestingen av
forskjellige monitoreringsverktøy skal foregå. Blade serveren har
betydelige ressurser som er i stand til å simulere et OpenStack miljø i
mye større grad enn det arbeidsstasjonen min til nå har hatt mulighet
for. Dette gjør testingen av potensielle verktøy enklere i tillegg til
at dataene jeg kommer til å teste med blir mest mulig reelle.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2015/03/2015-02-24-13.13.52.jpg"&gt;&lt;img alt="2015-02-24 13.13.52" src="http://openstack.b.uib.no/files/2015/03/2015-02-24-13.13.52-300x169.jpg" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2015/03/2015-02-24-13.13.37.jpg"&gt;&lt;img alt="2015-02-24 13.13.37" src="http://openstack.b.uib.no/files/2015/03/2015-02-24-13.13.37-300x169.jpg" /&gt;&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Fri, 06 Mar 2015 07:41:00 +0100</pubDate><guid>tag:blog.norcams.org,2015-03-06:dag-0-oppsummering.html</guid><category>bachelor</category><category>blade server</category><category>prosjekt</category><category>testing</category></item><item><title>Prosjektbeskrivelse bachelorprosjekt</title><link>http://blog.norcams.org/prosjektbeskrivelse-bachelorprosjekt.html</link><description>&lt;p&gt;Denne siden lister ut prosjektbeskrivelsen av bachelorprosjektet slik at
denne skal være enkel å finne frem til uansett hvor man er på bloggen.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2015/02/Prosjektbeskrivelse-bachelor-Kristian-Å.-Skurtveit.pdf"&gt;Prosjektbeskrivelse
bachelorprosjekt&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Tue, 03 Mar 2015 10:11:00 +0100</pubDate><guid>tag:blog.norcams.org,2015-03-03:prosjektbeskrivelse-bachelorprosjekt.html</guid></item><item><title>Prosjektbeskrivelse bacheloroppgave</title><link>http://blog.norcams.org/prosjektbeskrivelse-bacheloroppgave.html</link><description>&lt;p&gt;Etter høstens utplassering har jeg fått lov til å skrive
bacheloroppgaven min her på IT-avdelingen. I denne sammenhengen
publiserer jeg prosjektbeskrivelsen i sin helhet som omhandler
monitorering av OpenStack ved UiB. Jeg har også laget en ny side på
bloggen øverst til høyre som alltid linker til prosjektbeskrivelsen.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2015/02/Prosjektbeskrivelse-bachelor-Kristian-Å.-Skurtveit.pdf"&gt;Prosjektbeskrivelse
bachelorprosjekt&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Til nå har prosjektet kommet i gang og jeg har såvidt begynt å se på
forskjellige løsninger i henhold til problemstillingen som kan brukes
til å monitorere OpenStack. Videre vil jeg følge fremdriftsplanen i
prosjektbeskrivelsen. Noen verktøy jeg så langt har fått kikket på:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Icinga (basert på Nagios)&lt;/li&gt;
&lt;li&gt;Logstash&lt;/li&gt;
&lt;li&gt;Elasticsearch / Kibana&lt;/li&gt;
&lt;li&gt;Monasca&lt;/li&gt;
&lt;li&gt;Ceilometer / graphite&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Denne bloggen har tidligere vært brukt i utplasseringsfaget DAT156 og
har hatt en bloggpost for hver dag. Bloggen vil nå bli brukt til
bachelorprosjektet som varer frem til juni 2015. Og vil følge samme
oppsett med en bloggpost for hver dag. Neste blogginnlegg vil begynne på
dag 0 som oppsummerer arbeidet med oppgaven så langt.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Tue, 03 Mar 2015 10:03:00 +0100</pubDate><guid>tag:blog.norcams.org,2015-03-03:prosjektbeskrivelse-bacheloroppgave.html</guid><category>bachelor</category><category>prosjekt</category></item><item><title>Dag 33: "Skyen" ankommer UIB</title><link>http://blog.norcams.org/dag-33-skyen-ankommer-uib.html</link><description>&lt;p&gt;Siste dag på UiB i utplasseringsfaget hadde skyinfrastrukturen kommet på
plass i lageret. Dette er bare første del av to leveranser, men det er
på dette utstyret at skyen skal installeres.&lt;/p&gt;
&lt;p&gt;Det blir spennende tider fremover!&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://delldirekte.no/banebrytende-akademisk-skyprosjekt/"&gt;http://delldirekte.no/banebrytende-akademisk-skyprosjekt/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2014/11/2014-11-24-10.19.19.jpg"&gt;&lt;img alt="2014-11-24 10.19.19" src="http://openstack.b.uib.no/files/2014/11/2014-11-24-10.19.19-e1417086024925-576x1024.jpg" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2014/11/2014-11-24-10.19.36-e1417086051222.jpg"&gt;&lt;img alt="2014-11-24 10.19.36" src="http://openstack.b.uib.no/files/2014/11/2014-11-24-10.19.36-e1417086051222-576x1024.jpg" /&gt;&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Fri, 28 Nov 2014 18:20:00 +0100</pubDate><guid>tag:blog.norcams.org,2014-11-28:dag-33-skyen-ankommer-uib.html</guid><category>dell</category><category>infrastruktur</category><category>lager</category><category>openstack</category><category>sky</category></item><item><title>Dag 32: Dokumentasjon og OpenStack Monasca</title><link>http://blog.norcams.org/dag-32-dokumentasjon-og-openstack-monasca.html</link><description>&lt;p&gt;Fortsatte på dokumentasjonen av winch og hvordan man kan installere
OpenStack ved hjelp av manager noden og Foreman. Forøvrig kan alt av
dokumentasjonen rundt prosjektet leses på
&lt;a class="reference external" href="http://winch.readthedocs.org/en/latest/"&gt;readthedocs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I tillegg har jeg fått en konkret oppgave over jul til å kikke på
OpenStack monasca. Monasca er et åpent kildekodeverktøy som går under
prinsippet monitoring-as-a-service som overvåker OpenStack. Stort
verktøy som skal bli interessant å se på og installere. Dette blir også
et av monitoreringsverktøyene som jeg skal kikke på i forbindelse med
bachelorprosjektet neste år. Her er en liten oversikt over hvordan
Monasca integreres med OpenStack:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2014/11/Monasca-arch-component-diagram.png"&gt;&lt;img alt="Monasca-arch-component-diagram" src="http://openstack.b.uib.no/files/2014/11/Monasca-arch-component-diagram.png" /&gt;&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Fri, 28 Nov 2014 18:19:00 +0100</pubDate><guid>tag:blog.norcams.org,2014-11-28:dag-32-dokumentasjon-og-openstack-monasca.html</guid><category>readthedocs</category><category>retext</category><category>winch</category></item><item><title>Dag 31: Logstash &amp; Kibana</title><link>http://blog.norcams.org/dag-31-oppsett-av-logstash-kibana.html</link><description>&lt;p&gt;Benyttet mesteparten av dagen til å kikke på to verktøy som heter
&lt;a class="reference external" href="gstash.net"&gt;Logstash&lt;/a&gt;og
&lt;a class="reference external" href="http://www.elasticsearch.org/overview/kibana/"&gt;Kibana&lt;/a&gt;. Logstash er
et verktøy for å samle og håndtere logg. Den samler inn, parser og
lagrer logg og viser dem i et webpanel kalt Kibana. Her kan man ved
hjelp av et kraftig søkeverktøy søke opp alt av hendelser som har skjedd
på systemet siden loggingen startet.&amp;nbsp;I webpanelet vil det genereres
grafer på hvor ofte en hendelse man har søkt på forekommer. Ganske
nyttig dersom man skal filtrere en spesifikk feil for å finne ut om
feilen er kritisk eller ikke.&lt;/p&gt;
&lt;p&gt;Konfigurasjonsmessig kan man gjøre mye med logstash. Det viktigste av
konfigurasjonen skjer ved hjelp av en konfigurasjonsfil der en
spesifiserer input, hva som skal filtreres og output. Output delen kan
være forskjellige tjenester, i dette tilfellet ble output satt til å
være elasticsearch. Elasticsearch er logstash' sin backend for lagring
av logg. Senere vil jeg ta for meg installasjonen og oppsett av logstash
og kartlegge tjenester som skal overvåkes. I tillegg ønsker jeg også å
kikke nærmere på verktøyet
&lt;a class="reference external" href="http://graphite.wikidot.com/"&gt;Graphite&lt;/a&gt;&amp;nbsp;som kan spesifiseres som
output.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Wed, 26 Nov 2014 07:04:00 +0100</pubDate><guid>tag:blog.norcams.org,2014-11-26:dag-31-oppsett-av-logstash-kibana.html</guid><category>elasticsearch</category><category>graphite</category><category>kibana</category><category>logging</category></item><item><title>Dag 30: Monitorering av OpenStack ved hjelp av Icinga</title><link>http://blog.norcams.org/dag-30-monitorering-av-openstack-ved-hjelp-av-icinga.html</link><description>&lt;p&gt;Monitorering er et stort og viktig tema innenfor OpenStack og det finnes
mange forskjellige verktøy som kan benyttes. Jeg har så vidt begynt å
kikke på &lt;a class="reference external" href="https://www.icinga.org/"&gt;Icinga&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Icinga kan vise seg å være et nyttig verktøy for monitorering av
OpenStack. Verktøyet er i utgangspunktet basert på Nagios, derfor kan
mange av tilleggene som var skrevet for Nagios brukes direkte i Icinga.
For eksempel har
&lt;a class="reference external" href="https://github.com/stackforge/monitoring-for-openstack"&gt;Stackforge&lt;/a&gt;
en rekke scripts som kan brukes for å overvåke rene OpenStack tjenester.&lt;/p&gt;
&lt;p&gt;Samtidig har utviklerne bak Icinga støtte for å installere verktøyet ved
hjelp av puppet moduler, og tilbyr også vagrant bokser til testformål.
Siden resten av IaaS plattformen gjør dette per dags dato er Icinga
absolutt interessant å ha med videre som et aktuelt verktøy.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Fri, 21 Nov 2014 11:13:00 +0100</pubDate><guid>tag:blog.norcams.org,2014-11-21:dag-30-monitorering-av-openstack-ved-hjelp-av-icinga.html</guid><category>bachelor</category><category>icinga</category><category>monitorering</category><category>nagios</category><category>openstack</category><category>overvåkning</category></item><item><title>Dag 29: En bloggpost om oppetid og maskinvare og sånn...</title><link>http://blog.norcams.org/dag-29-en-bloggpost-om-oppetid-og-maskinvare-og-sann.html</link><description>&lt;p&gt;Dagene til den forrige kontor PC'en min var talte. 8GB just won't cut it
når man skal kjøre sky.&amp;nbsp; Hver av maskinene i OpenStack rammeverket ble
definert med 2GB RAM, og&amp;nbsp; ville en ha mer enn 4 maskiner ble det fort
lite minne igjen. Fikk&amp;nbsp; dermed ny arbeidsstasjon som hadde 16GB med RAM,
og provisjonering av OpenStack maskiner gikk nå unna fort som fy.&lt;/p&gt;
&lt;p&gt;Vi husker alle den første dagen og maskinen som lå på
&lt;a class="reference external" href="https://openstack.b.uib.no/2014/08/18/dag-1-installasjon-av-arbeidsplass-ved-uib/"&gt;hylla.&lt;/a&gt;Helt
til maskinen ble byttet ut hadde den siste tilgjengelige programvare,
den ble aldri startet om, og den fungerte utmerket i det aller fleste
tilfeller. Nå skal maskinen få nytt liv hjemme, og forhåpentligvis får
den et eget kabinett.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2014/11/uptime.png"&gt;&lt;img alt="uptime" src="http://openstack.b.uib.no/files/2014/11/uptime.png" /&gt;&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Fri, 21 Nov 2014 07:48:00 +0100</pubDate><guid>tag:blog.norcams.org,2014-11-21:dag-29-en-bloggpost-om-oppetid-og-maskinvare-og-sann.html</guid><category>oppetid</category></item><item><title>Dag 28: Bacheloroppgave ved UIB</title><link>http://blog.norcams.org/dag-28-bacheloroppgave-ved-uib.html</link><description>&lt;p&gt;Dette semesteret har gitt meg god innsikt i OpenStack og de forskjellige
komponentene rammeverket består av. &amp;nbsp;Siden jeg skal skrive
bacheloroppgaven min til neste år har jeg i denne sammenhengen fått lov
til å skrive den&amp;nbsp;her ved UIB.&lt;/p&gt;
&lt;p&gt;Jeg arbeider&amp;nbsp;for tiden med prosjektbeskrivelse til bachelorprosjektet.
Innledning, teori og metode er deler som foreløpig er ferdig i
prosjektbeskrivelsen, mens det fortsatt gjenstår noen deler på
problemstillingen. Jeg ønsker å ha en problemstilling som går ut på
monitorering av OpenStack. Dette er et stort og viktig tema der jeg har
muligheten til å gå i dybden av komponentene og finne ut hva tjenester
som er kritiske å overvåke kontra tjenester som ikke er det.&lt;/p&gt;
&lt;p&gt;Jeg vil ha mulighet til å teste ulike&amp;nbsp;monitoreringsverktøy og kartlegge
bruken av disse. Det vil være nyttig å finne ut av fordeler og ulemper
med enkelte verktøy og gjøre en vurdering på hvilke verktøy som vil være
hensiktsmessig å bruke i skysammenheng.&amp;nbsp;I tillegg vil jeg også se på
eventuelle etiske spørsmål og personvernspørsmål i forbindelse med
overvåkning av OpenStack.&lt;/p&gt;
&lt;p&gt;Fullført&amp;nbsp;prosjektbeskrivelse vil bli publisert her på bloggen når den er
klar.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Wed, 19 Nov 2014 06:24:00 +0100</pubDate><guid>tag:blog.norcams.org,2014-11-19:dag-28-bacheloroppgave-ved-uib.html</guid><category>bachelor</category><category>monitorering</category><category>overvåkning</category></item><item><title>Dag 27: Oppdatering av "winch" dokumentasjon</title><link>http://blog.norcams.org/dag-27-oppdatering-av-winch-dokumentasjon.html</link><description>&lt;p&gt;Mesteparten av dagen ble benyttet til å lese over dokumentasjonen til
&amp;quot;winch&amp;quot;.&amp;nbsp; Jeg har fått i oppgave å oppdatere dokumentasjonen slik at den
kan brukes til slik som &amp;quot;winch&amp;quot; er nå.&lt;/p&gt;
&lt;p&gt;I dette arbeidet kommer jeg til å bruke et verktøy som heter ReText.
Dette er et tekstredigeringsverktøy som gir direkte preview i programmet
for hvordan teksten du redigerer vil bli seende ut. I motsening til
andre editorer der en er tvunget av å måtte åpne siden i en nettleser
for å se hvordan den ser&amp;nbsp; ut.&lt;/p&gt;
&lt;p&gt;Dokumentasjonen finnes
på&lt;a class="reference external" href="//winch.readthedocs.org/"&gt;http://winch.readthedocs.org/&lt;/a&gt;, flere
oppdateringer vil skje de påfølgende ukene.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Fri, 14 Nov 2014 07:09:00 +0100</pubDate><guid>tag:blog.norcams.org,2014-11-14:dag-27-oppdatering-av-winch-dokumentasjon.html</guid><category>dokumentasjon</category><category>readthedocs</category><category>winch</category></item><item><title>Dag 25 &amp; 26: Samstemt provisjonering av ceph maskiner</title><link>http://blog.norcams.org/dag-25-26.html</link><description>&lt;p&gt;Fra forrige gang fant jeg ut at et Ceph kluster ikke kan provisjoneres
node for node. Dette ble også bekreftet av en av utviklerne fra
Stackforge. På bakgrunn av dette laget jeg et script som tar opp alle
nodene samtidig. I tillegg ble det lagt inn en 10 sekunders forsinkelse
mellom hver av maskinene slik at vi får korrekt IP adresse på dem.
Scriptet er et veldig enkelt script som kan sees i sin helhet på
&lt;a class="reference external" href="https://github.com/norcams/winch/blob/ceph/spawn-cephnodes.sh"&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Nå vil ikke timeout feilen fra forrige innlegg skje. Når den kommer til
punktet der nøklene skal sendes over til de andre maskinene i ceph
klusteret vil de andre nodene også ha kommet til dette steget. På denne
måten klarer nøklene å spres over til de andre maskinene og
provisjoneringen med puppet går gjennom uten feil.&lt;/p&gt;
&lt;p&gt;Videre arbeid blir nå å integrere ceph med resten av OpenStack slik at
instansene vi oppretter&amp;nbsp;bruker ceph som lagringsområde. I tillegg skal
jeg skrive en del dokumentasjon om winch og hvordan man tar det bedre i
bruk. Foreløpig dokumentasjon kan sees
på&amp;nbsp;&lt;a class="reference external" href="http://winch.readthedocs.org/en/latest/"&gt;http://winch.readthedocs.org/en/latest/&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Thu, 06 Nov 2014 09:41:00 +0100</pubDate><guid>tag:blog.norcams.org,2014-11-06:dag-25-26.html</guid><category>ceph</category><category>openstack</category><category>storage</category><category>winch</category></item><item><title>Dag 24: Feil med opprettelse av første monitoreringsnode</title><link>http://blog.norcams.org/dag-24-feil-med-opprettelse-av-forste-monitoreringsnode.html</link><description>&lt;p&gt;Ved opprettelse av første monitoreringsnode, ceph01 feiler kjøringen av
puppet modulene. Fra det manuelle oppsettet lærte vi at den første noden
setter inn nøkler og passord på de andre nodene i klusteret.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Error:
/Stage[main]/Ceph::Profile::Mon/Ceph::Key[client.bootstrap-osd]/Exec[ceph-injectkey-client.bootstrap-osd]/&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Når de to andre nodene ceph02 og ceph03 blir provisjonert så skjer ikke
denne feilen. Ting kan tyde på at nodene er svært avhenige av hverandre
når de skal provisjoneres. Ceph01 kan deretter provisjoneres på nytt
etter at andre og tredje node er kommet på. Da fungerer det uten
problemer og vi kan se utifra ceph -status at vi har et aktivt og
kjørende kluster.&lt;/p&gt;
&lt;blockquote&gt;
cluster 4b5c8c0a-ff60-454b-a1b4-9747aa737d19
health HEALTH_WARN clock skew detected on mon.ceph02, mon.ceph03
monmap e2: 3 mons at
{ceph01=172.16.33.13:6789/0,ceph02=172.16.33.14:6789/0,ceph03=172.16.33.15:6789/0},
election epoch 6, quorum 0,1,2 ceph01,ceph02,ceph03
osdmap e17: 6 osds: 6 up, 6 in
pgmap v24: 192 pgs, 3 pools, 0 bytes data, 0 objects
68720 MB used, 149 GB / 227 GB avail
192 active+clean&lt;/blockquote&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Wed, 05 Nov 2014 19:23:00 +0100</pubDate><guid>tag:blog.norcams.org,2014-11-05:dag-24-feil-med-opprettelse-av-forste-monitoreringsnode.html</guid><category>ceph</category><category>storage</category><category>winch</category></item><item><title>Dag 23: Installering av Ceph kluster med hjelp av puppet-moduler</title><link>http://blog.norcams.org/dag-23-installering-av-ceph-kluster-med-hjelp-av-puppet-moduler.html</link><description>&lt;p&gt;Det manuelle oppsettet av ceph-klusteret som ble installert forrige gang
fungerte bra. Planen for i dag var å lage et lignende kluster ved hjelp
av puppet moduler.&lt;/p&gt;
&lt;p&gt;Som vi husker fra tegningen jeg publiserte i forrige innlegg, fantes det
en egen monitoreringsnode og x antall osd noder. Ceph klusteret blir
spredt over alle osd nodene. Mens det er en monitoreringsnode som holder
styr på &lt;a class="reference external" href="http://ceph.com/docs/master/architecture/"&gt;klusteret&lt;/a&gt; og
hvor mange noder som er med osv.&amp;nbsp; For å hindre
&lt;a class="reference external" href="http://en.wikipedia.org/wiki/Single_point_of_failure"&gt;SPOF&lt;/a&gt; har vi i
et produksjonsmiljø lyst å kjøre flere monitoreringsnoder samtidig. I
tillegg ønsker vi å kjøre monitoreringsnoder og osd noder på samme
maskin. Dette for å minske behovet for ekstra maskiner.&lt;/p&gt;
&lt;p&gt;I node definisjonen i vagrant har vi lagt til 3 ceph noder; ceph01,
ceph02 og ceph03. Innstillinger for de 3 maskinene har blitt satt i
&lt;a class="reference external" href="https://github.com/norcams/winch/blob/ceph/puppet/hieradata/common.yaml"&gt;common.yaml&lt;/a&gt;
filen slik at vi kan provisjonere med puppet. Innstillingene til ceph
inneholder alt fra passord, medlemmer&amp;nbsp; på klusteret, størrelsen til
klusteret og hvor klusteret skal være på de medlemmene som er definert.
Dette vil nå bli satt opp automatisk for oss når vi installerer ceph
nodene.&lt;/p&gt;
&lt;p&gt;Etter dette kunne vi starte opp maskinene på samme måte som vi har gjort
tidligere. Klusteret innstalleres når maskinen kommer opp for første
gang. Mer om ceph neste gang.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Thu, 30 Oct 2014 13:15:00 +0100</pubDate><guid>tag:blog.norcams.org,2014-10-30:dag-23-installering-av-ceph-kluster-med-hjelp-av-puppet-moduler.html</guid></item><item><title>Dag 22: Installering av Ceph kluster</title><link>http://blog.norcams.org/dag-22-installering-av-ceph-kluster.html</link><description>&lt;p&gt;Agenda i dag var å få lest litt mer om filsystemet Ceph.
&lt;a class="reference external" href="http://ceph.com/"&gt;Ceph&lt;/a&gt; er et objekt basert filsystem som replikerer
sine data over et lagringskluster. Det tar vekk behovet for et
tradisjonelt SAN. Filsystemet ble utviklet i 2007, og ble i Mai i år
kjøpt opp av
&lt;a class="reference external" href="http://ceph.com/community/red-hat-to-acquire-inktank/"&gt;RedHat.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I prosjektet har vi lyst å deployere Ceph ved bruk av Puppet moduler.
For å få en så knirkefri installasjon som overhodet mulig kommer vi til
å bruke samme utviklere som laget OpenStack puppet modulene vi benyttet
tidligere. Stackforge utviklerne har mye spennende GitHub prosjekter
innenfor OpenStack og dens komponenter. For de som er interessert i å
sjekke ut Ceph repositoriet kan ta en kikk
&lt;a class="reference external" href="https://github.com/stackforge/puppet-ceph/"&gt;her&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I første omgang har vi deployert et Ceph kluster manuelt for å få et
innblikk i hvordan dette fungerer bit for bit før vi begynner å bruke
puppet modulene. Det er mye god dokumentasjon som ligger ute på
hjemmesidene til Ceph, vi begynte med følgende
&lt;a class="reference external" href="http://docs.ceph.com/docs/master/start/quick-start-preflight/"&gt;&amp;quot;quick-start-preflight&amp;quot;&lt;/a&gt;.
Deretter ble klusteret installert ved å følge
&lt;a class="reference external" href="http://docs.ceph.com/docs/master/start/quick-ceph-deploy/"&gt;&amp;quot;quick-start-deploy&amp;quot;.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Et kjørende kluster can for eksempel se slik ut:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2014/10/ceph-cluster.png"&gt;&lt;img alt="ceph-cluster" src="http://openstack.b.uib.no/files/2014/10/ceph-cluster.png" /&gt;&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Fri, 24 Oct 2014 06:58:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-10-24:dag-22-installering-av-ceph-kluster.html</guid><category>ceph</category><category>RedHat</category><category>Stackforge</category><category>storage</category></item><item><title>Dag 20 &amp; 21: Testing av Openstack deployment fra "winch"</title><link>http://blog.norcams.org/dag-20-21-testing-av-openstack-deployment-fra-winch.html</link><description>&lt;p&gt;I den siste tiden har det skjedd flere endringer på
&lt;a class="reference external" href="https://github.com/norcams/winch"&gt;&amp;quot;winch&amp;quot;&lt;/a&gt; prosjektet på Github. Vi
har nå et automatisert testmiljø som kan installere OpenStack og alle
dens komponenter uten å måtte sette mange innstillinger manuelt.&amp;nbsp; Dette
gir oss stort spillerom dersom noe ikke skulle virke med senere
anledninger. Da kan vi slette en komponent og installere den på nytt
igjen uten å tape mye tid.&lt;/p&gt;
&lt;p&gt;Samtidig vil det være lettere å lære seg de ulike OpenStack komponentene
når man kan se hvordan de fungerer sammen i praksis. Det er&amp;nbsp; lett å
miste oversikten når man begynner med rammeverket siden det virker
veldig overveldende og detaljert i starten.&lt;/p&gt;
&lt;p&gt;Dagens arbeid innebærte en god del testing, samt en endring i en av
installasjonsfilene. Vi kan nå konkludere med at &amp;quot;winch&amp;quot; er brukbart i
forhold til de tingene vi skal gjøre videre. Blant annet skal vi kikke
nærmere på storage, rettere sagt &lt;a class="reference external" href="http://ceph.com/"&gt;Ceph&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Fri, 17 Oct 2014 20:53:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-10-17:dag-20-21-testing-av-openstack-deployment-fra-winch.html</guid><category>ceph</category><category>openstack</category><category>storage</category><category>winch</category></item><item><title>Dag 19: Persistent montering av /vagrant/ mappe og netforward</title><link>http://blog.norcams.org/dag-19-persistent-montering-av-vagrant-mappe-og-netforward.html</link><description>&lt;p&gt;Som tidligere nevnt har det vært problemer ved bruk av vagrant at vi
mister /vagrant mappen på manager maskinen. I denne mappen ligger det en
del script for å installere og konfigurere Foreman og puppet. Da er det
veldig viktig at montering av denne mappen fungerer i alle tilfeller der
vi trenger den.&lt;/p&gt;
&lt;p&gt;Ved bruk av vagrant har dette vist seg å være vanskelig. Etter en
oppdatering av maskinen eller en restart utenfor vagrant miljøet blir
man tvunget til å kjøre kompilering av virtualbox addons og
kjernemoduler for å få monteringen opp igjen. Dette krever mye manuelt
arbeid og det ville vært lettere å automatisert prosessen. Derfor har
jeg laget et script som ordner dette. I tillegg til å montere mappen
kjører den også netforward på manager maskinen. Denne har tidligere falt
ut når manager har blitt startet om utenfor vagrant og er kritisk for at
Foreman fungerer slik vi vil.&lt;/p&gt;
&lt;p&gt;Scriptet kan sees i sin helhet på
&lt;a class="reference external" href="https://github.com/norcams/winch/blob/master/vagrant/manager-persistent-config.sh"&gt;github&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Mon, 13 Oct 2014 06:41:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-10-13:dag-19-persistent-montering-av-vagrant-mappe-og-netforward.html</guid><category>github</category><category>openstack</category><category>vagrant</category></item><item><title>Dag 18: Oppretting av virtualbox maskiner uten vagrant</title><link>http://blog.norcams.org/dag-18-oppretting-av-virtualbox-maskiner-uten-vagrant.html</link><description>&lt;p&gt;Onsdag 1. oktober var offisiel kickoff for UH-Sky prosjektet som jeg er
en del av. Fremover vil det være mye større aktivitet på github i form
av endringer enn tidligere. Til nå har arbeidet med &amp;quot;winch&amp;quot; vært preget
av mye feilsøking og manuell konfigurasjon. Det vi ønsker å oppnå er å
lage et helautomatisk oppsett for å deployere OpenStack i et testmiljø.&lt;/p&gt;
&lt;p&gt;For å unngå en del problemer vi har opplevd tidligere har vi besluttet å
lage et script som oppretter virtualbox maskiner uten innblanding fra
vagrant. Vagrant låser blant annet eth0 interfacet og setter dette til
et NAT'et interface. Dette har ført til at maskiner ikke får TFTP bootet
når skal provisjonere disse med Foreman. I tillegg har vi fått en del
linux bro problematikk og det har krevd mye feilsøking for å få ting til
å fungere som de skal. For nå har jeg laget et script som vil opprette
de to komponentene
&lt;a class="reference external" href="https://github.com/norcams/winch/blob/master/vagrant/create-vbox-compute.sh"&gt;compute&lt;/a&gt;og
&lt;a class="reference external" href="https://github.com/norcams/winch/blob/master/vagrant/create-vbox-controller.sh"&gt;controller&lt;/a&gt;.
Senere vil vi nok også utvide scriptet til å gjelde for storage og
network også.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Wed, 08 Oct 2014 11:41:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-10-08:dag-18-oppretting-av-virtualbox-maskiner-uten-vagrant.html</guid><category>compute</category><category>controller</category><category>foreman</category><category>vagrant</category><category>winch</category></item><item><title>Dag 17: Installasjon av compute og controller node ved hjelp av Foreman</title><link>http://blog.norcams.org/dag-17-installasjon-av-compute-og-controller-node-ved-hjelp-av-foreman.html</link><description>&lt;p&gt;Vi fortsatte med feilsøkingen fra fredag om hvorfor puppet modulene fra
&amp;quot;winch&amp;quot; ikke ville fungere når vi provisjonerte med Foreman. De hadde
virket før og det ga lite mening at de ikke skulle virke nå som Foreman
var med i bildet.&lt;/p&gt;
&lt;p&gt;Det viste seg at feilen lå spredt i flere konfigurasjonsfiler, og dette
gjorde at puppet modulene feilet på maskinene som Foreman installerte.
Nå har det blitt gjort endringer i kickstart filene til Foreman for å
gjøre at dette problemet ikke vil oppstå igjen. Nå var det mulig å
installere både controller og compute noder ved hjelp av Foreman der
alle puppet modulene blir installert.&lt;/p&gt;
&lt;p&gt;Til nå har vi brukt vagrant og virtualbox for å lage og provisjonere
maskiner. Vagrant legger opp et eth0 interface med en NAT adresse, men
dette kludrer mye til det nettverksoppsettet vi ønsker. Derfor er neste
steg nå å lage install script som kan lage og sette opp virtualbox
maskiner for å unngå at eth0 inferfacet blir bundet opp til denne NAT
adressen. Mer om dette på fredag.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Thu, 02 Oct 2014 10:43:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-10-02:dag-17-installasjon-av-compute-og-controller-node-ved-hjelp-av-foreman.html</guid><category>foreman</category><category>puppet</category><category>winch</category></item><item><title>Dag 15 &amp; 16: Testing og oppsett av Dell s4810 switcher og feilsøking av puppet moduler</title><link>http://blog.norcams.org/dag-15-16-testing-og-oppsett-av-dell-s4810-switcher.html</link><description>&lt;p&gt;Mandagen ble brukt til å konfigurere opp to nye &lt;a class="reference external" href="http://www.dell.com/learn/us/en/04/shared-content~data-sheets~en/documents~dell_force10_s4810_spec_sheet.pdf"&gt;Dell s4810
switcher&lt;/a&gt;
som skal brukes til testformål i prosjektet. I motsetning til andre
switcher som typisk kjører sitt eget operativsystem kjører disse
switchene på Linux. Cumulus Linux er en variant av Debian og det er
slike switcher som skal benyttes når den endelige skyplattformen
installeres. Oppsettet vi laget og monterte i rack kan visualiseres på
denne måten:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2014/09/iaas-ruting-redundans-test-Network-general-1.png"&gt;&lt;img alt="iaas ruting-redundans test - Network general-1" src="http://openstack.b.uib.no/files/2014/09/iaas-ruting-redundans-test-Network-general-1-1024x633.png" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I tillegg har vi også jobbet videre med Foreman. Det er en del problemer
som gjenstår før vi kan fullprovisjonere maskiner ved hjelp av puppet
moduler. Etter at Foreman har provisjonert en maskin sliter vi med å få
puppet modulene til å virke på maskinen. Fra tidligere fungerte modulene
etter vi hadde oppdatert dem i forbindelse med &amp;quot;winch&amp;quot; prosjektet.
Eneste endringen nå er at vi bruker Foreman. Videre feilsøking på
mandag...&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Mon, 29 Sep 2014 06:10:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-09-29:dag-15-16-testing-og-oppsett-av-dell-s4810-switcher.html</guid><category>cumulus linux</category><category>dell</category><category>gilgamesh</category><category>omo</category><category>openstack</category><category>switcher</category><category>winch</category></item><item><title>Dag 14: Oppdatering av Foreman og installasjon av compute noder</title><link>http://blog.norcams.org/dag-14-oppdatering-av-foreman-og-installasjon-av-compute-noder.html</link><description>&lt;p&gt;Vi startet dagen med å feilsøke på hvorfor
&lt;a class="reference external" href="http://en.wikipedia.org/wiki/Trivial_File_Transfer_Protocol"&gt;TFTP&lt;/a&gt;
serveren til Foreman ikke ville fungere. Flere problemer oppstod med
versjon 1.4.5 uten at vi fant noen gode forklaringer på hvorfor.
Maskinen som Foreman er installert på kjører en DHCP server og en TFTP
server. Disse tjenestene brukes når Foreman skal provisjonere opp
maskiner. Da blir maskinene PXE bootet og den henter ned det aktuelle
operativsystemet til maskinen og dette installeres automatisk.&lt;/p&gt;
&lt;p&gt;Når en eldre pakke byr på problemer som ovenfor er løsningen som oftest
å oppdatere til nyeste versjon. Alle installeringsscriptene til Foreman
ble derfor modifisert slik at scriptet henter den nyeste programvaren
fra repositoriet. Etter at Foreman hadde komt opp til versjon 1.6
fungerte TFTP serveren og det var nå mulig å provisjonere opp maskiner.
Dette ble testet både på en fysisk maskin og på en virtuell maskin.&lt;/p&gt;
&lt;p&gt;Når man skal provisjonere opp en maskin kan man velge fra en haug med
templater på hva innstillinger man ønsker osv. Foreman tilbyr mye
funskjonalitet og det er et stort verktøy å sette seg inn i. Heldigvis
er dette noe vi skal holde på med en god stund fremover, og det alltid
kjekt med bratte læringskurver.&lt;/p&gt;
&lt;p&gt;Eksempelutdrag på et Foreman dashboard hentet fra google:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2014/09/foreman.png"&gt;&lt;img alt="foreman" src="http://openstack.b.uib.no/files/2014/09/foreman.png" /&gt;&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Wed, 17 Sep 2014 15:33:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-09-17:dag-14-oppdatering-av-foreman-og-installasjon-av-compute-noder.html</guid><category>1.6</category><category>foreman</category><category>openstack</category><category>TFTP</category><category>winch</category></item><item><title>Dag 13: Installasjon av The Foreman</title><link>http://blog.norcams.org/dag-13-installasjon-av-the-foreman.html</link><description>&lt;p&gt;Etter å ha fått &amp;quot;winch&amp;quot; oppgradert til siste OpenStack versjon; Icehouse
started vi med installasjonen av &lt;a class="reference external" href="http://theforeman.org/"&gt;The
Foreman.&lt;/a&gt;Foreman ligger inne i &amp;quot;winch&amp;quot; med
versjon 1.4.5 og formålet med å installere denne versjonen er å kunne
verifisere at funksjonaliteten fungerer slik vi vil. Senere vil Foreman
bli oppgradert til versjon 1.6.0 .&lt;/p&gt;
&lt;p&gt;Foreman vil være en egen komponent i vår OpenStack installasjon og den
kjører derfor på en egen maskin, nemlig manager. Før installasjon av
Foreman må denne maskinen ha blitt laget. Inne i vagrant mappen på
&lt;a class="reference external" href="https://github.com/norcams/winch/blob/foreman/vagrant/foreman.sh"&gt;winch&lt;/a&gt;finnes
det et script som heter foreman.sh. Det er dette scriptet som kjøres når
Foreman installeres.&lt;/p&gt;
&lt;p&gt;Etter installasjon skjer alt av administrasjon inne fra et webpanel. Her
kan man sette opp puppet moduler, templater for installasjon og
provisjonering, diskoppsett med mer. Det finnes mange muligheter, og det
er dette verktøyet vi skal bruke i lag med OpenStack. Eksempelvis om man
skulle installere 40 servere ville man konfigurert Foreman til å gjøre
dette på en rask og effektiv måte. Alternativt måtte man gjort dette for
hver og enkelt server noe som ikke skalerer i lengden. På mandag vil vi
jobbe videre med The Foreman og sette igang med provisjonering av
compute noder.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Mon, 15 Sep 2014 06:16:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-09-15:dag-13-installasjon-av-the-foreman.html</guid><category>foreman</category><category>openstack</category><category>provisjonering</category><category>winch</category></item><item><title>Dag 12: Feilsøking av "winch" installasjon</title><link>http://blog.norcams.org/dag-12-feilsoking-av-winch-installasjon.html</link><description>&lt;p&gt;Fra tidligere har vi ofte hatt problemer med å få instanser til å snakke
med verden på utsiden. Dette har ofte krevd mer konfigurasjon og
feilsøking enn først antatt. Denne gangen var intet unntak. Det ble mye
repetering av OpenStack network in too much detail og sjekking av
brannmurregler for å verifisere at alt var i orden.&lt;/p&gt;
&lt;p&gt;Etter en god del feilsøking viste det seg til slutt å være at feilen
oppstod på grunn av
&lt;a class="reference external" href="http://en.wikipedia.org/wiki/Vagrant_%28software%29"&gt;vagrant&lt;/a&gt;. Vi
bruker vagrant til å lage virtuelle maskiner og denne hadde da stokket
om på IP adressene til de forskjellige interfacene maskinene skulle ha.
Derfor var det mye som ikke ble riktig og etterpå kunne maskinene komme
på nett. Senere i uken ble også &amp;quot;winch&amp;quot; prosjektet oppdatert. Endringer
kan sees &lt;a class="reference external" href="https://github.com/norcams/winch/tree/foreman"&gt;her&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Fri, 12 Sep 2014 06:27:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-09-12:dag-12-feilsoking-av-winch-installasjon.html</guid><category>feilsøking</category><category>nettverk</category><category>openstack</category><category>winch</category></item><item><title>Dag 11: Videre oppdatering av "winch" repository</title><link>http://blog.norcams.org/dag-11-videre-oppdatering-av-winch-repository.html</link><description>&lt;p&gt;Siden &amp;quot;winch&amp;quot; ble laget for å støtte en eldre versjon av OpenStack ble
det nødvendig å oppdatere flere av puppet modulene til å nyeste versjon.
Uten dette feilet installasjonen momentant og det var ikke mulig å
provisjonere opp maskiner med puppet når modulene var utdaterte. Etter
at disse ble oppdatert gikk installasjonen overraskende nok gjennom. Vi
klarte å sette opp en controller node og en compute node.&lt;/p&gt;
&lt;p&gt;Vi har tidligere holdt på med packstack som kjører alle OpenStack
komponentene på en og samme node. I et produksjonsmiljø vil vi heller
separere de ulike komponentene. Dette er av flere grunner; hver maskin
skal bare kunne gjøre det de er satt til, det vil lette trykket i
forhold til bare en og samme node, og det er mer oversiktlig og
skalerbart. Alle de forskjellige komponentene ligger under sine
respektive nett, her er en oversikt over hvordan det ser ut:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2014/09/RolesProfiles.png"&gt;&lt;img alt="RolesProfiles" src="http://openstack.b.uib.no/files/2014/09/RolesProfiles-1024x785.png" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Etter at controller og compute noden kom opp kjørte vi de forskjellige
testene som fulgte med winch. Disse testene verifiserer funksjonalitet
og setter samtidig opp nettverk, rutere, og instanser.&amp;nbsp; Nå har vi en
kjørende instans og selve oppsettet ser ut til å fungere greit. Opplever
nå at instansen ikke kan nås fra controller noden, så dette er noe vi må
feilsøke videre på mandag.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Mon, 08 Sep 2014 06:49:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-09-08:dag-11-videre-oppdatering-av-winch-repository.html</guid><category>openstack</category><category>puppet</category><category>winch</category></item><item><title>Dag 10: Feilsøking av instansnettverk &amp; installasjon av "winch"</title><link>http://blog.norcams.org/dag-10-feilsoking-av-instansnettverk-installasjon-av-winch.html</link><description>&lt;p&gt;Første del av dagen ble brukt til videre feilsøking av hvorfor
instansene våre ikke kunne snakke ut mot verden. Dette ble løst med å
spesifisere hvilket interface vi skulle kjøre maskering ifra. Iptables
regelen fra &lt;a class="reference external" href="https://openstack.redhat.com/Networking_in_too_much_detail"&gt;OpenStack networking in too
much&lt;/a&gt;
detail ble modifisert til å se slik ut:&lt;/p&gt;
&lt;blockquote&gt;
iptables -t nat -I POSTROUTING 1 -s 172.24.4.224/28 -o eth0 -j
MASQUERADE&lt;/blockquote&gt;
&lt;p&gt;Etter en iptables reload kunne instansene våre snakke ut mot verden.
Innenfor sine begrensede parametere kunne vi nå si at vi hadde en
fullverdig OpenStack installasjon.&lt;/p&gt;
&lt;p&gt;Resten av dagen ble benyttet til å se på
&lt;a class="reference external" href="https://github.com/norcams/winch"&gt;&amp;quot;winch&amp;quot;&lt;/a&gt; prosjektet fra GitHub.
Dette er et prosjekt på lik linje med OpenStack fra scratch. Formålet
med winch er å lære oss å deployere OpenStack ved hjelp av puppet
moduler og &lt;a class="reference external" href="http://theforeman.org/learn_more.html"&gt;The Forman&lt;/a&gt;. Kort
forklart er Forman en effektiv måte å installere all infrastruktur til
OpenStack installasjonen, eksempelvis compute, storage og network.
Foreman gir mye spennende funksjonalitet, så anbefaler videre lesning i
linken over. Når &amp;quot;winch&amp;quot; ble laget var OpenStack kommet til havana
versjonen. I disse dager bruker vi icehouse, som er samme versjon som
ble installert ved hjelp av packstack. For å kunne jobbe videre med
&amp;quot;winch&amp;quot; er vi først nødt til å oppdatere denne til å støtte icehouse.
Dette vil være klart til neste gang.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Fri, 05 Sep 2014 10:40:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-09-05:dag-10-feilsoking-av-instansnettverk-installasjon-av-winch.html</guid><category>foreman</category><category>iptables</category><category>winch</category></item><item><title>Dag 9:Tilbake til packstack --allinone og installasjon av fysisk controller</title><link>http://blog.norcams.org/dag-9tilbake-til-packstack-allinone-og-installasjon-av-fysisk-controller.html</link><description>&lt;p&gt;Etter at minnetildelingen hadde blitt økt flere ganger på vår virtuelle
controller node bestemte vi oss for å installere denne på en fysisk
maskin. I tillegg hadde vi også hatt problemer med neutron-server, så i
denne omgang gikk vi tilbake på å kjøre packstack --allinone. Dette
ville forhåpentligvis gjøre slik at vi unngikk flere problemer,
hvertfall nå som vi er i en testfase.&lt;/p&gt;
&lt;p&gt;Vi installerte på samme måte som tidligere. Fra før av har man en helt
ny installasjon av CentOS, og dermed følger man de &lt;a class="reference external" href="https://openstack.redhat.com/Quickstart"&gt;tre
stegene&lt;/a&gt;som jeg har
skrevet om tidligere
&lt;a class="reference external" href="http://openstack.b.uib.no/2014/08/21/dag-4-installasjon-av-openstack-packstack/"&gt;her&lt;/a&gt;.&amp;nbsp;Noen
av de tingene vi har slitt med fungerte med en gang. Både neutron-server
og det å lage instanser fungerte uten problemer.&lt;/p&gt;
&lt;p&gt;Det som gjenstår nå er å kunne få instansene til å snakke med verden på
utsiden. Dette gjør vi ved å utføre noen av de siste stegene i
&lt;a class="reference external" href="https://openstack.redhat.com/Networking_in_too_much_detail#NAT_to_host_address"&gt;OpenStack network in to much
detail&lt;/a&gt;.
Det blir satt&amp;nbsp;en offentlig IP adresse til OpenStack på br-ex interfacet.
Samtidig som vi legger til brannmurregler slik at trafikk på innsiden av
skyen blir maskert og videresendt gjennom noden og videre ut på &amp;nbsp;nettet.
Vi fikk litt problemer etter at reglene var lagt inn i brannmuren.
Instansene klarer ikke å pinge&amp;nbsp;ut i verden. Det eneste instansen klarer
å pinge er controller noden og vice versa.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2014/08/Neutron_architecture.png"&gt;&lt;img alt="Neutron_architecture" src="http://openstack.b.uib.no/files/2014/08/Neutron_architecture-300x166.png" /&gt;&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Wed, 03 Sep 2014 06:03:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-09-03:dag-9tilbake-til-packstack-allinone-og-installasjon-av-fysisk-controller.html</guid><category>allinone</category><category>feilsøking</category><category>nettverk</category><category>openstack</category><category>packstack</category></item><item><title>Dag 8: Videre feilsøking av neutron-server</title><link>http://blog.norcams.org/dag-8-videre-feilsoking-av-neutron-server-og-installasjon-av-fysisk-controller-node.html</link><description>&lt;p&gt;På denne dagen gikk mye av tiden med til å feilsøke hvorfor
neutron-server ikke ville starte på compute noden. Feilsøking uten å ha
noe logg å lese kan ofte være vanskelig, og da kommer det helt&amp;nbsp;an på
hvilke svar man kan finne på nettet i lignende installasjoner. Til slutt
viste det seg at en av de tilleggene som neutron bruker ikke fantes på
den stien den skulle ha vært, og tillegget var heller ikke installert.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
yum install openstack-neutron-ml2
&lt;/pre&gt;
&lt;p&gt;I tillegg skal det være en plugin.ini fil under /etc/neutron/ som skal
holde rede på hvilke tillegg man har lagt til installasjonen av
OpenStack. Denne må symlinkes hertil.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
&lt;/pre&gt;
&lt;p&gt;Etter dette fungerte det å starte neutron-server tjenesten. Foruten om
dette har det vært noen stabilitetsproblemer på controller noden. Denne
har til nå kjørt på en virtuell maskin og har flere ganger fått økt
minnetildeling. I skrivende stund kjører den på 4GB RAM, halvparten av
det jeg har på min arbeidsstasjon. Jeg har opptil flere ganger hatt
stabilitetsproblemer ved å kjøre denne maskinen virtuelt. En del ganger
har jeg blitt nødt til å kjøre power down eller reset for å få kontakt
med maskinen.&amp;nbsp;På grunn av dette er planen til neste gang å få lagt
controller noden over på en fysisk maskin. Samtidig skal vi også
installere ML2 tillegget før vi kjører packstack, slik at vi ikke får de
samme problemene med neutron-server som tidligere.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Thu, 28 Aug 2014 08:34:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-08-28:dag-8-videre-feilsoking-av-neutron-server-og-installasjon-av-fysisk-controller-node.html</guid><category>compute</category><category>controller</category><category>feilsøking</category><category>openstack</category><category>plugin</category></item><item><title>Dag 7: OpenStack networking in too much detail &amp; feilsøking av neutron-server</title><link>http://blog.norcams.org/dag-7-openstack-network-in-too-much-detail-feilsoking-av-neutron-server.html</link><description>&lt;p&gt;Etter at en virtuell maskin har blitt startet i en instans er vi
interessert i å kunne snakke med denne maskinen utenfra OpenStack
nettverket. Det er her OpenStack in to much detail kommer inn i bildet.
&amp;nbsp;Se bildet nedenfor for mer detaljer.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2014/08/Neutron_architecture.png"&gt;&lt;img alt="Neutron_architecture" src="http://openstack.b.uib.no/files/2014/08/Neutron_architecture-300x166.png" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Jeg vil ikke prøve å forklare dette i detalj da nettverket er veldig
komplekst.
&lt;a class="reference external" href="http://openstack.redhat.com/Networking_in_too_much_detail"&gt;Artikkelen&lt;/a&gt;&amp;nbsp;tar
utgangspunkt i å forklare arkitekturen, og hvordan nettverk, bruer og
tunneler henger sammen i en OpenStack installasjon. Bildet er forøvrig
skrevet ut og ligger ved arbeidsstasjonen min, regner med at det skal
repeteres en god del ganger før nettverket sitter 100%.&lt;/p&gt;
&lt;p&gt;Etter at vi hadde fått laget instanser ville ikke nettverket i en
instans starte slik vi ville. Etter litt feilsøking fant vi ut at
neutron-server tjenesten på compute noden ikke starter. Tjenesten dør
momentant når den blir startet og det skjer såpass fort at det ikke er
noe innslag i loggen som kan si hva som er galt. Resten av dagen gikk
med til å feilsøke hvorfor neutron-server ikke ville starte.
Initscriptet er likt som på controller noden, og
/etc/neutron/neutron.conf er identisk på de to maskinene.&amp;nbsp;Dette må
feilsøkes ytterligere, fortsettelse følger på mandag.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Wed, 27 Aug 2014 06:22:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-08-27:dag-7-openstack-network-in-too-much-detail-feilsoking-av-neutron-server.html</guid><category>compute</category><category>neutron</category><category>openstack</category></item><item><title>Dag 6: Installering av fysisk compute node</title><link>http://blog.norcams.org/dag-6-installering-av-fysisk-compute-node.html</link><description>&lt;p&gt;18. august hadde vi fått installert compute noden på en fysisk maskin.
Til nå hadde vi hatt problemer med å lage og starte instanser når alt
kjørte på en node, på grunn av lite tilgjengelige ressurser. Som
tidligere forklart kunne vi nå bruke packstack svarfilen som ville gjøre
installsjonen for oss. Det eneste man oppgir er root passordet til
maskinen man skal installere på.&lt;/p&gt;
&lt;blockquote&gt;
**** Installation completed successfully ******&lt;/blockquote&gt;
&lt;p&gt;Etter installasjonen får man oppgitt adressen til
&lt;a class="reference external" href="https://wiki.openstack.org/wiki/Horizon"&gt;Horizon&lt;/a&gt; der man kan logge
inn og administrere skyen. På dette tidspunktet klarte vi å logge inn og
lage instanser.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2014/08/lage-instans.png"&gt;&lt;img alt="lage-instans" src="http://openstack.b.uib.no/files/2014/08/lage-instans-300x274.png" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Det er de tilgjengelige ressursene man har på compute noden som vil
sette begrensninger på hvor mange instanser man kan lage. Man kan velge
størrelse på maskinen og hvor mange instanser av maskinen som skal
starte. Utifra de verdiene man spesifiserer vil ressursene under project
limits endre seg. I verktøyet kan man spesifisere sikkerhet, hva
nettverk maskinen skal ha, hva post-scripts som skal kjøres og om man
vil kjøre automatisk eller manuelt diskoppsett. Alt i alt er det et
veldig oversiktlig og funksjonelt verktøy som det skal bli veldig kjekt
å lære mer om!&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Fri, 22 Aug 2014 18:24:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-08-22:dag-6-installering-av-fysisk-compute-node.html</guid><category>compute</category><category>horizon</category><category>openstack</category></item><item><title>Dag 5: Prøving og feiling med packstack --allinone</title><link>http://blog.norcams.org/dag-5-proving-og-feiling-med-packstack-allinone.html</link><description>&lt;p&gt;15 august. De tre hovedkomponentene til OpenStack hadde nå blitt
installert med packstack til å kjøre på en og samme node. &amp;nbsp;Dette burde
ikke by på store problemer da RDO pakken fra Red Hat er godt testet og
fungerer i mange installasjoner av OpenStack der ute i dag.&lt;/p&gt;
&lt;p&gt;Alle instanser som kjører innad i OpenStack rammeverket kan
administreres via et webpanel kalt
&lt;a class="reference external" href="https://wiki.openstack.org/wiki/Horizon"&gt;Horizon&lt;/a&gt;. Panelet gir en
god oversikt over antall instanser, deres ressursbruk, last, og annen
info. På sikt skal både administratorer og vanlige brukere kunne logge
seg inn her og administrere sine egne maskiner til forskjellige&amp;nbsp;formål.
Et lite utdrag fra Horizon:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2014/08/Horizon-overview.png"&gt;&lt;img alt="Horizon-overview" src="http://openstack.b.uib.no/files/2014/08/Horizon-overview-300x132.png" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Vi fikk fortsatt problemer med å lage og starte instanser med å kjøre de
tre hovedkomponentene på en og samme node. Mistenker at dette kan ha med
lite tilgjengelige ressurser siden vi kjører på en virtuell maskin
gjennom VirtualBox. For å løse denne problematikken ble svaret å lage en
ekstra compute node&amp;nbsp;på en fysisk maskin. Med dette ville vi unngå
problematikken vi hadde hatt til nå, i tillegg til at vi kan lage flere
maskiner siden vi har mer ressurser tilgjengelig.&lt;/p&gt;
&lt;p&gt;For å installere en egen compute node trenger man, (i vårt tilfelle) en
nyinstallert &lt;a class="reference external" href="http://en.wikipedia.org/wiki/CentOS"&gt;CentOS&lt;/a&gt;&amp;nbsp;samt
packstack-answer filen vi brukte når vi installerte hoved noden. Da vil
packstack bruke de samme passordene til de ulike komponentene og compute
noden vil peke tilbake til controller noden slik at disse vil snakke
sammen.&lt;/p&gt;
&lt;p&gt;Mye av installasjon av compute noden er lik som forrige gang. Når alt i
konfigurasjonsfilen er klart blir det kjørt på samme måte som tidligere
med en liten endring. Istedenfor:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
packstack --allinone
&lt;/pre&gt;
&lt;p&gt;Blir det nå spesifisert en svarfil som brukes i installasjonen:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre class="literal-block"&gt;
packstack --answer-file=$youranswerfile
&lt;/pre&gt;
&lt;/blockquote&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Thu, 21 Aug 2014 18:11:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-08-21:dag-5-proving-og-feiling-med-packstack-allinone.html</guid><category>centos</category><category>openstack</category><category>packstack</category><category>rdo</category></item><item><title>Dag 4: Installasjon av OpenStack packstack</title><link>http://blog.norcams.org/dag-4-installasjon-av-openstack-packstack.html</link><description>&lt;p&gt;11. august var satt av til å installere OpenStack fra RDO.&amp;nbsp;RDO er under
utvikling av Red Hat og er en egen utgivelse av OpenStack laget
spesifikt for å kjøre
på&amp;nbsp;&lt;a class="reference external" href="http://en.wikipedia.org/wiki/Red_Hat_Enterprise_Linux"&gt;RHEL&lt;/a&gt;,
&lt;a class="reference external" href="http://en.wikipedia.org/wiki/CentOS"&gt;CentOS&lt;/a&gt;,&amp;nbsp;&lt;a class="reference external" href="http://en.wikipedia.org/wiki/Fedora_(operating_system)"&gt;Fedora&lt;/a&gt;&amp;nbsp;og
andre Red Hat baserte systemer. &amp;nbsp;For mer info om RDO vennligst se
&lt;a class="reference external" href="http://openstack.redhat.com/FAQ"&gt;her&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Installasjon av RDO foregår ved hjelp av packstack. Packstack er et
installasjonsverktøy som bruker Puppet moduler for å&amp;nbsp;installere
OpenStack på&amp;nbsp;kompatible maskiner nevnt ovenfor. Har i denne posten valgt
å ta med steg-for-steg installasjonen av RDO for å vise hvordan dette
gjøres, se nedenfor for mer detaljer.&lt;/p&gt;
&lt;p&gt;Vanligvis består OpenStack av tre hovedkomponenter; network, storage og
compute, som hver kjører på sin egen maskin.&amp;nbsp;Når man installerer
packstack kan man velge at disse tre hovedkomponentene skal installeres
på en og samme maskin. Dette kan være praktisk av flere grunner, i vårt
tilfelle er det greit for å se hvordan det fungerer for testformål. Vi
kan senere også velge å separere hovedkomponentene slik vi ønsker.&lt;/p&gt;
&lt;p&gt;RDO installeres med &lt;strong&gt;3&lt;/strong&gt; steg. Hentet
fra&amp;nbsp;&lt;a class="reference external" href="http://openstack.redhat.com/Quickstart"&gt;http://openstack.redhat.com/Quickstart&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steg 1: Programvare&amp;nbsp;repository:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Oppdater nåværende pakker på systemet:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre class="literal-block"&gt;
sudo yum update -y
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Sett opp RDO repositoriet:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre class="literal-block"&gt;
sudo yum install -y http://rdo.fedorapeople.org/rdo-release.rpm
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Steg 2: Installer packstack&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre class="literal-block"&gt;
sudo yum install -y openstack-packstack
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Steg 3: Kjør packstack for å installere OpenStack&lt;/p&gt;
&lt;p&gt;Packstack installerer automatisk OpenStack slik at man slipper å
gjøre&amp;nbsp;denne jobben manuelt. For å kjøre OpenStack på en enkelt node kan
vi som tidligere nevnt kjøre denne kommandoen:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre class="literal-block"&gt;
packstack --allinone
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Etter installering vil packstack også generere en packstack-answer fil
med passord og innstillinger som den bruker under installasjon. Denne
kan så brukes om igjen til nye installasjoner eller til å lage nye noder
med samme innstillinger. Om man installerer på andre maskiner vil man
bli bedt om root passordet til maskinen før installasjonen starter.&lt;/p&gt;
&lt;p&gt;Etter installasjon vil man kunne logge inn på panelet
&lt;a class="reference external" href="https://wiki.openstack.org/wiki/Horizon"&gt;Horizon&lt;/a&gt;for å starte og
administrere de instanser man måtte ønske. Mer om dette videre.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Thu, 21 Aug 2014 10:58:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-08-21:dag-4-installasjon-av-openstack-packstack.html</guid><category>centos</category><category>fedora</category><category>openstack</category><category>packstack</category><category>rdo</category><category>rhel</category></item><item><title>Dag 3: Videre feilsøking av OpenStack from scratch</title><link>http://blog.norcams.org/dag-3-videre-feilsoking-av-openstack-from-scratch.html</link><description>&lt;p&gt;8. august gikk mye av tiden til å feilsøke på hvorfor installasjonen fra
gårsdagen ikke ville lage instanser. &amp;nbsp;OpenStack from scratch har en
rekke tester en kan kjøre når&amp;nbsp;installasjonen er ferdig. Testene ligger
her:&amp;nbsp;&lt;a class="reference external" href="https://github.com/norcams/ofs/tree/master/tests"&gt;https://github.com/norcams/ofs/tree/master/tests&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For eksempel vil
&lt;a class="reference external" href="https://github.com/norcams/ofs/blob/master/tests/01-import_image.sh"&gt;import_image.sh&lt;/a&gt;&amp;nbsp;importere
et image som den virtuelle maskinen i instansen vil starte fra. Dette
for å verifisere at glance image service fra OpenStack fungerer som det
skal. I vårt tilfelle feilet
&lt;a class="reference external" href="https://github.com/norcams/ofs/blob/master/tests/04-boot.sh"&gt;04-boot.sh&lt;/a&gt;&amp;nbsp;som
bruker komponenten nova til å starte den virtuelle maskinen.&amp;nbsp;Det ble
etterhvert mye lesing i nova loggene for å finne ut av feilen. Jeg hadde
funnet frem til flere alternative løsninger fra nettet for å gjøre ting
i en annen rekkefølge. Samt at jeg reinstallerte de tre
&amp;nbsp;hovedkomponentene network storage og compute for å teste ut noen av
løsningene.&amp;nbsp;Dette ga ikke noen resultater.&lt;/p&gt;
&lt;p&gt;Det har uansett vært interessant å ha installert dette fra bunnen av,
selv om vi visste på forhånd at det ville være noen feil med
installasjonen. Jeg har blitt fortrolig med flere av kommandoene på de
verktøyene som er i bruk samtidig som jeg har fått et greit overblikk på
hvordan komponentene fungerer sammen.&amp;nbsp;I neste uke har vi tenkt å gå
videre med en annen installasjon av OpenStack
nemlig&amp;nbsp;&lt;a class="reference external" href="http://openstack.redhat.com/Frequently_Asked_Questions#What_is_RDO.3F"&gt;RDO&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Wed, 20 Aug 2014 11:59:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-08-20:dag-3-videre-feilsoking-av-openstack-from-scratch.html</guid><category>feilsøking</category><category>glance</category><category>nova</category><category>ofs</category></item><item><title>Dag 2: OpenStack from scratch</title><link>http://blog.norcams.org/dag-2-openstack-from-scratch.html</link><description>&lt;p&gt;7. august gikk med til å utføre en OpenStack installasjon fra scratch.
Denne installasjonen gir et godt inntrykk av hva komponenter OpenStack
består av og hvordan disse skal fungere sammen i en komplett
installasjon.&lt;/p&gt;
&lt;p&gt;Vi støtte på litt problemer etter installasjonen av OpenStack from
scratch. Noe&amp;nbsp;av hovedfunksjonaliteten i&amp;nbsp;systemet er å kunne lage og
bygge instanser av maskiner som skal kjøre i et driftsmiljø. Denne biten
feilet og vi har startet feilsøking av hva som kan ha gått galt. Dette
er noe vi kommer til å fortsette med i morgen.&lt;/p&gt;
&lt;p&gt;OpenStack from scratch er et github prosjekt&amp;nbsp;som er blitt laget i
forbindelse med prosjektet jeg er med i på UIB. &amp;nbsp;Hele&amp;nbsp;prosjektet ligger
åpent på github -&amp;gt;&amp;nbsp;&lt;a class="reference external" href="https://github.com/norcams/ofs"&gt;https://github.com/norcams/ofs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Før jeg begynner å gå inn i detaljer om OpenStack, vil det være greit
for leserne å vite litt om bakgrunnen til programvaren og hva den brukes
til. Velger i denne sammenhengen å vise til
&lt;a class="reference external" href="http://en.wikipedia.org/wiki/Openstack"&gt;Wikipedia&lt;/a&gt;artikkelen som
jeg syns har oppsummert dette ganske bra, samt at den forklarer bruken
til hver av komponentene.&lt;/p&gt;
&lt;p&gt;&lt;img alt="OpenStack" src="http://openstack.b.uib.no/files/2014/08/OpenStack.png" /&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Tue, 19 Aug 2014 08:57:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-08-19:dag-2-openstack-from-scratch.html</guid><category>github</category><category>ofs</category></item><item><title>Dag 1: Installasjon av arbeidsplass ved UIB</title><link>http://blog.norcams.org/dag-1-installasjon-av-arbeidsplass-ved-uib.html</link><description>&lt;p&gt;Fredag 4. august var første arbeidsdag på IT-avdelingen ved UIB. Jeg har
tidligere jobbet her som lærling og ble godt tatt imot av både gamle og
nye kolleger. Jeg skal jobbe med Iaas (Infrastructure as a service) og
mine arbeidsoppgaver vil omfatte nettverk, lagring, compute og
automasjon innenfor OpenStack rammeverket.&lt;/p&gt;
&lt;p&gt;Som første dager flest gikk mesteparten av tiden til å installere
arbeidsstasjonen samt å finne to skjermer som ville passe til.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2014/08/2014-08-01-14.43.38.jpg"&gt;&lt;img alt="2014-08-01 14.43.38" src="http://openstack.b.uib.no/files/2014/08/2014-08-01-14.43.38-300x168.jpg" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I mangel på kabinett ble arbeidsstasjonen min seende slik ut:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://openstack.b.uib.no/files/2014/08/2014-08-18-14.09.08.jpg"&gt;&lt;img alt="2014-08-18 14.09.08" src="http://openstack.b.uib.no/files/2014/08/2014-08-18-14.09.08-300x168.jpg" /&gt;&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristian Skurtveit</dc:creator><pubDate>Mon, 18 Aug 2014 12:28:00 +0200</pubDate><guid>tag:blog.norcams.org,2014-08-18:dag-1-installasjon-av-arbeidsplass-ved-uib.html</guid><category>openstack</category><category>pc</category><category>uib</category></item></channel></rss>